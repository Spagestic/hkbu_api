{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "apiKey = os.getenv(\"HKBU_API_KEY\")\n",
    "\n",
    "if apiKey is None:\n",
    "    raise ValueError(\"HKBU_API_KEY not found in environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicUrl = \"https://genai.hkbu.edu.hk/general/rest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_models = [\n",
    "    {\n",
    "        \"model\": \"gpt-4-o\",\n",
    "        'api-version': '2024-10-21',\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4-o-mini\",\n",
    "        'api-version': '2024-10-21',\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"o1-preview\",\n",
    "        'api-version': '2024-10-21',\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"o1-mini\",\n",
    "        'api-version': '2024-10-21',\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"text-embedding-3-large\",\n",
    "        'api-version': '2024-05-01-preview',\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"text-embedding-3-small\",\n",
    "        'api-version': '2024-05-01-preview',\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenAI(\n",
    "        message: str,\n",
    "        model_name: str = \"gpt-4-o-mini\",\n",
    "        imageURL: str = None,\n",
    "        temperature: float = 0,\n",
    "        max_tokens: int = 100,\n",
    "        tools: list = None,\n",
    "        stream: bool = False\n",
    "        # response_format: dict = None,\n",
    "        ):\n",
    "    # Find the model in the openai_models list\n",
    "    model_info = next((model for model in openai_models if model[\"model\"] == model_name), None)\n",
    "    \n",
    "    if not model_info:\n",
    "        raise ValueError(f\"Model {model_name} not found in openai_models list\")\n",
    "    \n",
    "    api_version = model_info['api-version']\n",
    "    \n",
    "    conversation = [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    if imageURL:\n",
    "        conversation[0][\"content\"] = [\n",
    "            {\"type\": \"text\", \"text\": message},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": imageURL, \"detail\": \"low\"}}\n",
    "        ]\n",
    "    \n",
    "    url = basicUrl + \"/deployments/\" + model_name + \"/chat/completions/?api-version=\" + api_version\n",
    "    headers = { 'Content-Type': 'application/json', 'api-key': apiKey }\n",
    "    payload = { \n",
    "        'messages': conversation,\n",
    "        'temperature': temperature,\n",
    "        'max_tokens': max_tokens,\n",
    "        'tools': tools,\n",
    "        \"stream\": stream,\n",
    "        # \"response_format\": response_format\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        return 'Error:', response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-B3KKTKNxY4FRy5GdCHAV7uTSAiNlV\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1740132829,\n",
      "    \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Hello! How can I assist you today?\",\n",
      "                \"refusal\": null\n",
      "            },\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 8,\n",
      "        \"completion_tokens\": 9,\n",
      "        \"total_tokens\": 17,\n",
      "        \"prompt_tokens_details\": {\n",
      "            \"cached_tokens\": 0,\n",
      "            \"audio_tokens\": 0\n",
      "        },\n",
      "        \"completion_tokens_details\": {\n",
      "            \"reasoning_tokens\": 0,\n",
      "            \"audio_tokens\": 0,\n",
      "            \"accepted_prediction_tokens\": 0,\n",
      "            \"rejected_prediction_tokens\": 0\n",
      "        }\n",
      "    },\n",
      "    \"system_fingerprint\": \"fp_b705f0c291\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = OpenAI(\n",
    "    message=\"hello\", \n",
    "    model_name=\"gpt-4-o-mini\", \n",
    "    imageURL=None, \n",
    "    temperature=0.5, \n",
    "    max_tokens=20,\n",
    "    tools=None,\n",
    "    stream=True,\n",
    "    response_format=\"text\"\n",
    "    )\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_models = [\n",
    "    {\n",
    "        \"model\": \"claude-3-5-sonnet\",\n",
    "        \"api-version\": \"20240620\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"claude-3-haiku\",\n",
    "        \"api-version\": \"20240307\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Claude(\n",
    "        message, \n",
    "        model_name=\"claude-3-5-sonnet\", \n",
    "        imageURL=None, \n",
    "        temperature=0, \n",
    "        max_tokens=100\n",
    "        ):\n",
    "    # Find the model in the claude_models list\n",
    "    model_info = next((model for model in claude_models if model[\"model\"] == model_name), None)\n",
    "    \n",
    "    if not model_info:\n",
    "        raise ValueError(f\"Model {model_name} not found in claude_models list\")\n",
    "    \n",
    "    api_version = model_info['api-version']\n",
    "    \n",
    "    conversation = [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    if imageURL:\n",
    "        conversation[0][\"content\"] = [\n",
    "            {\"type\": \"text\", \"text\": message},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": imageURL, \"detail\": \"low\"}}\n",
    "        ]\n",
    "    \n",
    "    url = basicUrl + \"/deployments/\" + model_name + \"/messages/?api-version=\" + api_version\n",
    "    headers = { 'Content-Type': 'application/json', 'api-key': apiKey }\n",
    "    payload = { \n",
    "        'messages': conversation,\n",
    "        'temperature': temperature,\n",
    "        'max_tokens': max_tokens\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        return 'Error:', response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 429 - {\"error\":{\"message\":\"Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: anthropic-claude-3-haiku. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\",\"param\":null,\"code\":429}}\n"
     ]
    }
   ],
   "source": [
    "result = Claude(message=\"hello\", model_name=\"claude-3-haiku\", imageURL=None, temperature=0.5, max_tokens=20)\n",
    "\n",
    "if isinstance(result, tuple) and result[0] == 'Error:':\n",
    "\tprint(f\"Error: {result[1].status_code} - {result[1].text}\")\n",
    "else:\n",
    "\tprint(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_models = [\n",
    "    {\n",
    "        \"model\": \"gemini-1.5-pro\",\n",
    "        \"api-version\": \"002\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gemini-1.5-flash\",\n",
    "        \"api-version\": \"002\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gemini(message, model_name=\"gemini-1.5-flash\", temperature=0.7, maxOutputTokens=10, stream=False):\n",
    "    # Find the model in the gemini_models list\n",
    "    model_info = next((model for model in gemini_models if model[\"model\"] == model_name), None)\n",
    "    \n",
    "    if not model_info:\n",
    "        raise ValueError(f\"Model {model_name} not found in gemini_models list\")\n",
    "    \n",
    "    api_version = model_info['api-version']\n",
    "    \n",
    "    contents = [{\"role\": \"user\", \"parts\": [{\"text\": message}]}]\n",
    "    \n",
    "    url = f\"{basicUrl}/deployments/{model_name}/generate_content?api-version={api_version}\"\n",
    "    headers = { \n",
    "        'Content-Type': 'application/json', \n",
    "        'accept': 'application/json',\n",
    "        'api-key': apiKey \n",
    "    }\n",
    "    payload = { \n",
    "        'contents': contents,\n",
    "        'generationConfig': {\n",
    "            'maxOutputTokens': maxOutputTokens,\n",
    "            'temperature': temperature\n",
    "        },\n",
    "        'stream': stream\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print('Error:', response)\n",
    "        print('Payload:', json.dumps(payload, indent=2))\n",
    "        print('Response Text:', response.text)\n",
    "        print('URL:', url)\n",
    "        print('Headers:', headers)\n",
    "        return 'Error:', response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"candidates\": [\n",
      "        {\n",
      "            \"content\": {\n",
      "                \"role\": \"model\",\n",
      "                \"parts\": [\n",
      "                    {\n",
      "                        \"text\": \"Hello there! How can I help you today?\"\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"finishReason\": \"MAX_TOKENS\",\n",
      "            \"avgLogprobs\": -0.0006957607809454202,\n",
      "            \"index\": 0\n",
      "        }\n",
      "    ],\n",
      "    \"usageMetadata\": {\n",
      "        \"promptTokenCount\": 1,\n",
      "        \"candidatesTokenCount\": 10,\n",
      "        \"totalTokenCount\": 11,\n",
      "        \"promptTokensDetails\": [\n",
      "            {\n",
      "                \"modality\": \"TEXT\",\n",
      "                \"tokenCount\": 1\n",
      "            }\n",
      "        ],\n",
      "        \"candidatesTokensDetails\": [\n",
      "            {\n",
      "                \"modality\": \"TEXT\",\n",
      "                \"tokenCount\": 10\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"modelVersion\": \"gemini-1.5-flash-002\",\n",
      "    \"createTime\": \"2025-02-21T09:21:16.058832Z\",\n",
      "    \"responseId\": \"jEW4Z9DLA4PP-dIPubTK0A8\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = Gemini(message=\"hello\", model_name=\"gemini-1.5-flash\", temperature=0.5, maxOutputTokens=10, stream=False)\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_models = [\n",
    "    {\n",
    "        \"model\": \"llama3_1\",\n",
    "        \"api-version\": \"20240723\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama(message, model_name=\"llama3_1\", imageURL=None, temperature=0, max_tokens=100, top_p=1.0, top_k=50, stop_sequences=None, stream=False, system=None):\n",
    "    # Find the model in the claude_models list\n",
    "    model_info = next((model for model in llama_models if model[\"model\"] == model_name), None)\n",
    "    \n",
    "    if not model_info:\n",
    "        raise ValueError(f\"Model {model_name} not found in claude_models list\")\n",
    "    \n",
    "    api_version = model_info['api-version']\n",
    "    \n",
    "    conversation = [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    if imageURL:\n",
    "        conversation[0][\"content\"] = [\n",
    "            {\"type\": \"text\", \"text\": message},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": imageURL, \"detail\": \"low\"}}\n",
    "        ]\n",
    "    \n",
    "    url = basicUrl + \"/deployments/\" + model_name + \"/llama/completion/?api-version=\" + api_version\n",
    "    headers = { 'Content-Type': 'application/json', 'api-key': apiKey }\n",
    "    payload = { \n",
    "        'messages': conversation,\n",
    "        'temperature': temperature,\n",
    "        'max_tokens': max_tokens,\n",
    "        'top_p': top_p,\n",
    "        'top_k': top_k,\n",
    "        'stop_sequences': stop_sequences,\n",
    "        'stream': stream,\n",
    "        'system': system\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        return 'Error:', response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"Hello! How are you today? Is there something I can help you with or would you like to chat?\",\n",
      "                \"role\": \"assistant\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1740129678,\n",
      "    \"id\": \"2025-02-21|01:21:18.256198-08|2.65.20.36|1964221016\",\n",
      "    \"model\": \"meta/llama-3.1-405b-instruct-maas\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"system_fingerprint\": \"\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 22,\n",
      "        \"prompt_tokens\": 1,\n",
      "        \"total_tokens\": 23\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = llama(\n",
    "    message=\"hello\", \n",
    "    model_name=\"llama3_1\", \n",
    "    imageURL=None, \n",
    "    temperature=0.5, \n",
    "    max_tokens=10, \n",
    "    top_p=1.0, \n",
    "    top_k=50, \n",
    "    stop_sequences=None, \n",
    "    stream=False, \n",
    "    system=None\n",
    "    )\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\"model\": \"gpt-4-o\", \"api-version\": \"2024-10-21\", \"type\": \"openai\"},\n",
    "    {\"model\": \"gpt-4-o-mini\", \"api-version\": \"2024-10-21\", \"type\": \"openai\"},\n",
    "    {\"model\": \"o1-preview\", \"api-version\": \"2024-10-21\", \"type\": \"openai\"},\n",
    "    {\"model\": \"o1-mini\", \"api-version\": \"2024-10-21\", \"type\": \"openai\"},\n",
    "    {\"model\": \"text-embedding-3-large\", \"api-version\": \"2024-05-01-preview\", \"type\": \"openai\"},\n",
    "    {\"model\": \"text-embedding-3-small\", \"api-version\": \"2024-05-01-preview\", \"type\": \"openai\"},\n",
    "    {\"model\": \"claude-3-5-sonnet\", \"api-version\": \"20240620\", \"type\": \"claude\"},\n",
    "    {\"model\": \"claude-3-haiku\", \"api-version\": \"20240307\", \"type\": \"claude\"},\n",
    "    {\"model\": \"gemini-1.5-pro\", \"api-version\": \"002\", \"type\": \"gemini\"},\n",
    "    {\"model\": \"gemini-1.5-flash\", \"api-version\": \"002\", \"type\": \"gemini\"},\n",
    "    {\"model\": \"llama3_1\", \"api-version\": \"20240723\", \"type\": \"llama\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(message, model_name, kwargs):\n",
    "    model_info = next((model for model in models if model[\"model\"] == model_name), None)\n",
    "    if not model_info:\n",
    "        raise ValueError(f\"Model {model_name} not found\")\n",
    "\n",
    "    api_version = model_info['api-version']\n",
    "    model_type = model_info['type']\n",
    "\n",
    "    headers = {'Content-Type': 'application/json', 'api-key': apiKey}\n",
    "\n",
    "    if model_type == \"openai\":\n",
    "        url = f\"{basicUrl}/deployments/{model_name}/chat/completions/?api-version={api_version}\"\n",
    "        payload = {\n",
    "            'messages': [{\"role\": \"user\", \"content\": message}],\n",
    "            'temperature': kwargs.get('temperature', 0),\n",
    "            'max_tokens': kwargs.get('max_tokens', 100)\n",
    "        }\n",
    "        if kwargs.get('imageURL'):\n",
    "             payload['messages'][0]['content'] = [\n",
    "                {\"type\": \"text\", \"text\": message},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": kwargs.get('imageURL'), \"detail\": \"low\"}}\n",
    "            ]\n",
    "    elif model_type == \"claude\":\n",
    "        url = f\"{basicUrl}/deployments/{model_name}/messages/?api-version={api_version}\"\n",
    "        payload = {\n",
    "            'messages': [{\"role\": \"user\", \"content\": message}],\n",
    "            'temperature': kwargs.get('temperature', 0),\n",
    "            'max_tokens': kwargs.get('max_tokens', 100)\n",
    "        }\n",
    "        if kwargs.get('imageURL'):\n",
    "             payload['messages'][0]['content'] = [\n",
    "                {\"type\": \"text\", \"text\": message},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": kwargs.get('imageURL'), \"detail\": \"low\"}}\n",
    "            ]\n",
    "    elif model_type == \"gemini\":\n",
    "        url = f\"{basicUrl}/deployments/{model_name}/generate_content?api-version={api_version}\"\n",
    "        headers['accept'] = 'application/json'  # Gemini requires this header\n",
    "        payload = {\n",
    "            'contents': [{\"role\": \"user\", \"parts\": [{\"text\": message}]}],\n",
    "            'generationConfig': {\n",
    "                'maxOutputTokens': kwargs.get('maxOutputTokens', 10),\n",
    "                'temperature': kwargs.get('temperature', 0.7)\n",
    "            },\n",
    "            'stream': kwargs.get('stream', False)\n",
    "        }\n",
    "    elif model_type == \"llama\":\n",
    "        url = f\"{basicUrl}/deployments/{model_name}/llama/completion/?api-version={api_version}\"\n",
    "        payload = {\n",
    "            'messages': [{\"role\": \"user\", \"content\": message}],\n",
    "            'temperature': kwargs.get('temperature', 0),\n",
    "            'max_tokens': kwargs.get('max_tokens', 100),\n",
    "            'top_p': kwargs.get('top_p', 1.0),\n",
    "            'top_k': kwargs.get('top_k', 50),\n",
    "            'stop_sequences': kwargs.get('stop_sequences', None),\n",
    "            'stream': kwargs.get('stream', False),\n",
    "            'system': kwargs.get('system', None)\n",
    "        }\n",
    "        if kwargs.get('imageURL'):\n",
    "             payload['messages'][0]['content'] = [\n",
    "                {\"type\": \"text\", \"text\": message},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": kwargs.get('imageURL'), \"detail\": \"low\"}}\n",
    "            ]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "query_model() got an unexpected keyword argument 'maxOutputTokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mquery_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-1.5-flash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxOutputTokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(result, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m query_model(message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: query_model() got an unexpected keyword argument 'maxOutputTokens'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "result = query_model(message=\"hello\", model_name=\"gemini-1.5-flash\", temperature=0.5, maxOutputTokens=6)\n",
    "print(json.dumps(result, indent=4))\n",
    "\n",
    "result = query_model(message=\"hello\", model_name=\"gpt-4-o-mini\", temperature=0.5, max_tokens=6)\n",
    "print(json.dumps(result, indent=4))\n",
    "\n",
    "result = query_model(message=\"hello\", model_name=\"llama3_1\", temperature=0.5, max_tokens=6)\n",
    "print(json.dumps(result, indent=4))\n",
    "\n",
    "result = query_model(message=\"hello\", model_name=\"claude-3-haiku\", temperature=0.5, max_tokens=6)\n",
    "print(json.dumps(result, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hkbu_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
