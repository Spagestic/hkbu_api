{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "apiKey = os.getenv(\"HKBU_API_KEY\")\n",
    "basicUrl = os.getenv(\"HKBU_BASIC_URL\")\n",
    "if apiKey is None:\n",
    "    raise ValueError(\"HKBU_API_KEY not found in environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_models = [\n",
    "    {\n",
    "        \"model\": \"gpt-4-o\",\n",
    "        'api-version': '2024-10-21',\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4-o-mini\",\n",
    "        'api-version': '2024-10-21',\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"o1-preview\",\n",
    "        'api-version': '2024-10-21',\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"o1-mini\",\n",
    "        'api-version': '2024-10-21',\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"text-embedding-3-large\",\n",
    "        'api-version': '2024-05-01-preview',\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"text-embedding-3-small\",\n",
    "        'api-version': '2024-05-01-preview',\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenAI(\n",
    "        message: str,\n",
    "        model_name: str = \"gpt-4-o-mini\",\n",
    "        imageURL: str = None,\n",
    "        temperature: float = 0,\n",
    "        max_tokens: int = 100,\n",
    "        tools: list = None,\n",
    "        stream: bool = False\n",
    "        # response_format: dict = None,\n",
    "        ):\n",
    "    # Find the model in the openai_models list\n",
    "    model_info = next((model for model in openai_models if model[\"model\"] == model_name), None)\n",
    "    \n",
    "    if not model_info:\n",
    "        raise ValueError(f\"Model {model_name} not found in openai_models list\")\n",
    "    \n",
    "    api_version = model_info['api-version']\n",
    "    \n",
    "    conversation = [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    if imageURL:\n",
    "        conversation[0][\"content\"] = [\n",
    "            {\"type\": \"text\", \"text\": message},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": imageURL, \"detail\": \"low\"}}\n",
    "        ]\n",
    "    \n",
    "    url = basicUrl + \"/deployments/\" + model_name + \"/chat/completions/?api-version=\" + api_version\n",
    "    headers = { 'Content-Type': 'application/json', 'api-key': apiKey }\n",
    "    payload = { \n",
    "        'messages': conversation,\n",
    "        'temperature': temperature,\n",
    "        'max_tokens': max_tokens,\n",
    "        'tools': tools,\n",
    "        \"stream\": stream,\n",
    "        # \"response_format\": response_format\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        return 'Error:', response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-B3KKTKNxY4FRy5GdCHAV7uTSAiNlV\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1740132829,\n",
      "    \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Hello! How can I assist you today?\",\n",
      "                \"refusal\": null\n",
      "            },\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 8,\n",
      "        \"completion_tokens\": 9,\n",
      "        \"total_tokens\": 17,\n",
      "        \"prompt_tokens_details\": {\n",
      "            \"cached_tokens\": 0,\n",
      "            \"audio_tokens\": 0\n",
      "        },\n",
      "        \"completion_tokens_details\": {\n",
      "            \"reasoning_tokens\": 0,\n",
      "            \"audio_tokens\": 0,\n",
      "            \"accepted_prediction_tokens\": 0,\n",
      "            \"rejected_prediction_tokens\": 0\n",
      "        }\n",
      "    },\n",
      "    \"system_fingerprint\": \"fp_b705f0c291\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = OpenAI(\n",
    "    message=\"hello\", \n",
    "    model_name=\"gpt-4-o-mini\", \n",
    "    imageURL=None, \n",
    "    temperature=0.5, \n",
    "    max_tokens=20,\n",
    "    tools=None,\n",
    "    stream=True,\n",
    "    response_format=\"text\"\n",
    "    )\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_models = [\n",
    "    {\n",
    "        \"model\": \"claude-3-5-sonnet\",\n",
    "        \"api-version\": \"20240620\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"claude-3-haiku\",\n",
    "        \"api-version\": \"20240307\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Claude(\n",
    "        message, \n",
    "        model_name=\"claude-3-5-sonnet\", \n",
    "        imageURL=None, \n",
    "        temperature=0, \n",
    "        max_tokens=100\n",
    "        ):\n",
    "    # Find the model in the claude_models list\n",
    "    model_info = next((model for model in claude_models if model[\"model\"] == model_name), None)\n",
    "    \n",
    "    if not model_info:\n",
    "        raise ValueError(f\"Model {model_name} not found in claude_models list\")\n",
    "    \n",
    "    api_version = model_info['api-version']\n",
    "    \n",
    "    conversation = [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    if imageURL:\n",
    "        conversation[0][\"content\"] = [\n",
    "            {\"type\": \"text\", \"text\": message},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": imageURL, \"detail\": \"low\"}}\n",
    "        ]\n",
    "    \n",
    "    url = basicUrl + \"/deployments/\" + model_name + \"/messages/?api-version=\" + api_version\n",
    "    headers = { 'Content-Type': 'application/json', 'api-key': apiKey }\n",
    "    payload = { \n",
    "        'messages': conversation,\n",
    "        'temperature': temperature,\n",
    "        'max_tokens': max_tokens\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        return 'Error:', response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 429 - {\"error\":{\"message\":\"Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: anthropic-claude-3-haiku. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\",\"param\":null,\"code\":429}}\n"
     ]
    }
   ],
   "source": [
    "result = Claude(message=\"hello\", model_name=\"claude-3-haiku\", imageURL=None, temperature=0.5, max_tokens=20)\n",
    "\n",
    "if isinstance(result, tuple) and result[0] == 'Error:':\n",
    "\tprint(f\"Error: {result[1].status_code} - {result[1].text}\")\n",
    "else:\n",
    "\tprint(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_models = [\n",
    "    {\n",
    "        \"model\": \"gemini-1.5-pro\",\n",
    "        \"api-version\": \"002\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gemini-1.5-flash\",\n",
    "        \"api-version\": \"002\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gemini(\n",
    "    message: str,\n",
    "    model_name: str = \"gemini-1.5-flash\",\n",
    "    temperature: float = 0.5,\n",
    "    maxOutputTokens: int = 10,\n",
    "    response_schema: dict = None\n",
    "):\n",
    "    # Find the model in the gemini_models list\n",
    "    model_info = next((model for model in gemini_models if model[\"model\"] == model_name), None)\n",
    "    \n",
    "    if not model_info:\n",
    "        raise ValueError(f\"Model {model_name} not found in gemini_models list\")\n",
    "    \n",
    "    api_version = model_info['api-version']\n",
    "    \n",
    "    url = f\"{basicUrl}/deployments/{model_name}/generate_content?api-version={api_version}\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'accept': 'application/json',\n",
    "        'api-key': apiKey\n",
    "    }\n",
    "    \n",
    "    # Define the contents with the user's message\n",
    "    contents = [{\"role\": \"user\", \"parts\": [{\"text\": message}]}]\n",
    "    \n",
    "    # Define the payload with the generation config and optional response schema\n",
    "    payload = {\n",
    "        'contents': contents,\n",
    "        'generationConfig': {\n",
    "            'maxOutputTokens': maxOutputTokens,\n",
    "            'temperature': temperature,\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "        },\n",
    "        'stream': False\n",
    "    }\n",
    "    \n",
    "    if response_schema:\n",
    "        payload['generationConfig']['response_schema'] = response_schema\n",
    "    \n",
    "    # Make the POST request to the API\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    \n",
    "    # Check the response status code\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"candidates\": [\n",
      "        {\n",
      "            \"content\": {\n",
      "                \"role\": \"model\",\n",
      "                \"parts\": [\n",
      "                    {\n",
      "                        \"text\": \"[{\\\"ingredients\\\": [\\\"1 cup (2 sticks) unsalted butter, softened\\\", \\\"1 1/2 cups granulated sugar\\\", \\\"1 cup packed brown sugar\\\", \\\"2 large eggs\\\", \\\"2 teaspoons vanilla extract\\\", \\\"3 cups all-purpose flour\\\", \\\"1 teaspoon baking soda\\\", \\\"1 teaspoon salt\\\", \\\"1 cup chocolate chips\\\", \\\"1 cup chopped nuts (optional)\\\"], \\\"recipe_name\\\": \\\"Chocolate Chip Cookies\\\"}, {\\\"ingredients\\\": [\\\"1 cup (2 sticks) unsalted butter, softened\\\", \\\"1 cup granulated sugar\\\", \\\"1/2 cup packed brown sugar\\\", \\\"2 large eggs\\\", \\\"1 teaspoon vanilla extract\\\", \\\"2 1/4 cups all-purpose flour\\\", \\\"1 teaspoon baking soda\\\", \\\"1/2 teaspoon salt\\\", \\\"1/2 cup unsweetened cocoa powder\\\"], \\\"recipe_name\\\": \\\"Chocolate Cookies\\\"}, {\\\"ingredients\\\": [\\\"1/2 cup (1 stick) unsalted butter, softened\\\", \\\"1/2 cup granulated sugar\\\", \\\"1/2 cup packed brown sugar\\\", \\\"1 large egg\\\", \\\"1 teaspoon vanilla extract\\\", \\\"1 1/4 cups all-purpose flour\\\", \\\"1/2 teaspoon baking soda\\\", \\\"1/4 teaspoon salt\\\", \\\"1/2 cup peanut butter\\\", \\\"1/2 cup chocolate chips\\\"], \\\"recipe_name\\\": \\\"Peanut Butter Cookies\\\"}]\"\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"finishReason\": \"STOP\",\n",
      "            \"citationMetadata\": {\n",
      "                \"citations\": [\n",
      "                    {\n",
      "                        \"startIndex\": 109,\n",
      "                        \"endIndex\": 261,\n",
      "                        \"uri\": \"https://commandame.com/urban-cookhouse-half-baked-cookie-recipe/\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"startIndex\": 438,\n",
      "                        \"endIndex\": 584,\n",
      "                        \"uri\": \"https://github.com/Ganesh010914/Foodies-Paradise-\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"startIndex\": 664,\n",
      "                        \"endIndex\": 834,\n",
      "                        \"uri\": \"https://vocal.media/education/7-utimate-air-fryer-recipies\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"startIndex\": 727,\n",
      "                        \"endIndex\": 898,\n",
      "                        \"uri\": \"https://cupofsugarpinchofsalt.com/2015/01/16/pistachio-and-dark-chocolate-chip-cookies-with-bourbon-smoked-sea-salt/\"\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"avgLogprobs\": -0.017803099225549138,\n",
      "            \"index\": 0\n",
      "        }\n",
      "    ],\n",
      "    \"usageMetadata\": {\n",
      "        \"promptTokenCount\": 18,\n",
      "        \"candidatesTokenCount\": 272,\n",
      "        \"totalTokenCount\": 290,\n",
      "        \"promptTokensDetails\": [\n",
      "            {\n",
      "                \"modality\": \"TEXT\",\n",
      "                \"tokenCount\": 18\n",
      "            }\n",
      "        ],\n",
      "        \"candidatesTokensDetails\": [\n",
      "            {\n",
      "                \"modality\": \"TEXT\",\n",
      "                \"tokenCount\": 272\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"modelVersion\": \"gemini-1.5-flash-002\",\n",
      "    \"createTime\": \"2025-02-21T11:38:17.060576Z\",\n",
      "    \"responseId\": \"qWW4Z6DZA7KElMsP5avliQU\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Define the structured schema for the response\n",
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"recipe_name\": {\"type\": \"STRING\"},\n",
    "            \"ingredients\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}},\n",
    "        },\n",
    "        \"required\": [\"recipe_name\", \"ingredients\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"\"\"\n",
    "    List a few popular cookie recipes.\n",
    "\"\"\"\n",
    "\n",
    "# Query the Gemini model with the structured schema\n",
    "result = Gemini(\n",
    "    message=prompt,\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    temperature=0.5,\n",
    "    maxOutputTokens=500,\n",
    "    response_schema=response_schema\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ingredients': ['1 cup (2 sticks) unsalted butter, softened',\n",
       "   '1 1/2 cups granulated sugar',\n",
       "   '1 cup packed brown sugar',\n",
       "   '2 large eggs',\n",
       "   '2 teaspoons vanilla extract',\n",
       "   '3 cups all-purpose flour',\n",
       "   '1 teaspoon baking soda',\n",
       "   '1 teaspoon salt',\n",
       "   '1 cup chocolate chips',\n",
       "   '1 cup chopped nuts (optional)'],\n",
       "  'recipe_name': 'Chocolate Chip Cookies'},\n",
       " {'ingredients': ['1 cup (2 sticks) unsalted butter, softened',\n",
       "   '1 cup granulated sugar',\n",
       "   '1/2 cup packed brown sugar',\n",
       "   '2 large eggs',\n",
       "   '1 teaspoon vanilla extract',\n",
       "   '2 1/4 cups all-purpose flour',\n",
       "   '1 teaspoon baking soda',\n",
       "   '1/2 teaspoon salt',\n",
       "   '1/2 cup unsweetened cocoa powder'],\n",
       "  'recipe_name': 'Chocolate Cookies'},\n",
       " {'ingredients': ['1/2 cup (1 stick) unsalted butter, softened',\n",
       "   '1/2 cup granulated sugar',\n",
       "   '1/2 cup packed brown sugar',\n",
       "   '1 large egg',\n",
       "   '1 teaspoon vanilla extract',\n",
       "   '1 1/4 cups all-purpose flour',\n",
       "   '1/2 teaspoon baking soda',\n",
       "   '1/4 teaspoon salt',\n",
       "   '1/2 cup peanut butter',\n",
       "   '1/2 cup chocolate chips'],\n",
       "  'recipe_name': 'Peanut Butter Cookies'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ingredients\": [\n",
      "        \"1 cup (2 sticks) unsalted butter, softened\",\n",
      "        \"1 1/2 cups granulated sugar\",\n",
      "        \"1 cup packed brown sugar\",\n",
      "        \"2 large eggs\",\n",
      "        \"2 teaspoons vanilla extract\",\n",
      "        \"3 cups all-purpose flour\",\n",
      "        \"1 teaspoon baking soda\",\n",
      "        \"1 teaspoon salt\",\n",
      "        \"1 cup chocolate chips\",\n",
      "        \"1 cup chopped nuts (optional)\"\n",
      "    ],\n",
      "    \"recipe_name\": \"Chocolate Chip Cookies\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "recipes = json.loads(result)\n",
    "first_recipe = recipes[0]\n",
    "print(json.dumps(first_recipe, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chocolate Chip Cookies\n"
     ]
    }
   ],
   "source": [
    "print(first_recipe[\"recipe_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"1 cup (2 sticks) unsalted butter, softened\",\n",
      "    \"1 1/2 cups granulated sugar\",\n",
      "    \"1 cup packed brown sugar\",\n",
      "    \"2 large eggs\",\n",
      "    \"2 teaspoons vanilla extract\",\n",
      "    \"3 cups all-purpose flour\",\n",
      "    \"1 teaspoon baking soda\",\n",
      "    \"1 teaspoon salt\",\n",
      "    \"1 cup chocolate chips\",\n",
      "    \"1 cup chopped nuts (optional)\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(first_recipe[\"ingredients\"], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 cup (2 sticks) unsalted butter, softened'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_recipe[\"ingredients\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_models = [\n",
    "    {\n",
    "        \"model\": \"llama3_1\",\n",
    "        \"api-version\": \"20240723\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama(message, model_name=\"llama3_1\", imageURL=None, temperature=0, max_tokens=100, top_p=1.0, top_k=50, stop_sequences=None, stream=False, system=None):\n",
    "    # Find the model in the claude_models list\n",
    "    model_info = next((model for model in llama_models if model[\"model\"] == model_name), None)\n",
    "    \n",
    "    if not model_info:\n",
    "        raise ValueError(f\"Model {model_name} not found in claude_models list\")\n",
    "    \n",
    "    api_version = model_info['api-version']\n",
    "    \n",
    "    conversation = [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    if imageURL:\n",
    "        conversation[0][\"content\"] = [\n",
    "            {\"type\": \"text\", \"text\": message},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": imageURL, \"detail\": \"low\"}}\n",
    "        ]\n",
    "    \n",
    "    url = basicUrl + \"/deployments/\" + model_name + \"/llama/completion/?api-version=\" + api_version\n",
    "    headers = { 'Content-Type': 'application/json', 'api-key': apiKey }\n",
    "    payload = { \n",
    "        'messages': conversation,\n",
    "        'temperature': temperature,\n",
    "        'max_tokens': max_tokens,\n",
    "        'top_p': top_p,\n",
    "        'top_k': top_k,\n",
    "        'stop_sequences': stop_sequences,\n",
    "        'stream': stream,\n",
    "        'system': system\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        return 'Error:', response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"Hello! How are you today? Is there something I can help you with or would you like to chat?\",\n",
      "                \"role\": \"assistant\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1740129678,\n",
      "    \"id\": \"2025-02-21|01:21:18.256198-08|2.65.20.36|1964221016\",\n",
      "    \"model\": \"meta/llama-3.1-405b-instruct-maas\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"system_fingerprint\": \"\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 22,\n",
      "        \"prompt_tokens\": 1,\n",
      "        \"total_tokens\": 23\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = llama(\n",
    "    message=\"hello\", \n",
    "    model_name=\"llama3_1\", \n",
    "    imageURL=None, \n",
    "    temperature=0.5, \n",
    "    max_tokens=10, \n",
    "    top_p=1.0, \n",
    "    top_k=50, \n",
    "    stop_sequences=None, \n",
    "    stream=False, \n",
    "    system=None\n",
    "    )\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\"model\": \"gpt-4-o\", \"api-version\": \"2024-10-21\", \"type\": \"openai\"},\n",
    "    {\"model\": \"gpt-4-o-mini\", \"api-version\": \"2024-10-21\", \"type\": \"openai\"},\n",
    "    {\"model\": \"o1-preview\", \"api-version\": \"2024-10-21\", \"type\": \"openai\"},\n",
    "    {\"model\": \"o1-mini\", \"api-version\": \"2024-10-21\", \"type\": \"openai\"},\n",
    "    {\"model\": \"text-embedding-3-large\", \"api-version\": \"2024-05-01-preview\", \"type\": \"openai\"},\n",
    "    {\"model\": \"text-embedding-3-small\", \"api-version\": \"2024-05-01-preview\", \"type\": \"openai\"},\n",
    "    {\"model\": \"claude-3-5-sonnet\", \"api-version\": \"20240620\", \"type\": \"claude\"},\n",
    "    {\"model\": \"claude-3-haiku\", \"api-version\": \"20240307\", \"type\": \"claude\"},\n",
    "    {\"model\": \"gemini-1.5-pro\", \"api-version\": \"002\", \"type\": \"gemini\"},\n",
    "    {\"model\": \"gemini-1.5-flash\", \"api-version\": \"002\", \"type\": \"gemini\"},\n",
    "    {\"model\": \"llama3_1\", \"api-version\": \"20240723\", \"type\": \"llama\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(\n",
    "        message, \n",
    "        model_name, \n",
    "        kwargs\n",
    "        ):\n",
    "    model_info = next((model for model in models if model[\"model\"] == model_name), None)\n",
    "    if not model_info:\n",
    "        raise ValueError(f\"Model {model_name} not found\")\n",
    "\n",
    "    api_version = model_info['api-version']\n",
    "    model_type = model_info['type']\n",
    "\n",
    "    headers = {'Content-Type': 'application/json', 'api-key': apiKey}\n",
    "\n",
    "    if model_type == \"openai\":\n",
    "        url = f\"{basicUrl}/deployments/{model_name}/chat/completions/?api-version={api_version}\"\n",
    "        payload = {\n",
    "            'messages': [{\"role\": \"user\", \"content\": message}],\n",
    "            'temperature': kwargs.get('temperature', 0),\n",
    "            'max_tokens': kwargs.get('max_tokens', 100)\n",
    "        }\n",
    "        if kwargs.get('imageURL'):\n",
    "             payload['messages'][0]['content'] = [\n",
    "                {\"type\": \"text\", \"text\": message},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": kwargs.get('imageURL'), \"detail\": \"low\"}}\n",
    "            ]\n",
    "    elif model_type == \"claude\":\n",
    "        url = f\"{basicUrl}/deployments/{model_name}/messages/?api-version={api_version}\"\n",
    "        payload = {\n",
    "            'messages': [{\"role\": \"user\", \"content\": message}],\n",
    "            'temperature': kwargs.get('temperature', 0),\n",
    "            'max_tokens': kwargs.get('max_tokens', 100)\n",
    "        }\n",
    "        if kwargs.get('imageURL'):\n",
    "             payload['messages'][0]['content'] = [\n",
    "                {\"type\": \"text\", \"text\": message},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": kwargs.get('imageURL'), \"detail\": \"low\"}}\n",
    "            ]\n",
    "    elif model_type == \"gemini\":\n",
    "        url = f\"{basicUrl}/deployments/{model_name}/generate_content?api-version={api_version}\"\n",
    "        headers['accept'] = 'application/json'  # Gemini requires this header\n",
    "        payload = {\n",
    "            'contents': [{\"role\": \"user\", \"parts\": [{\"text\": message}]}],\n",
    "            'generationConfig': {\n",
    "                'maxOutputTokens': kwargs.get('maxOutputTokens', 10),\n",
    "                'temperature': kwargs.get('temperature', 0.7)\n",
    "            },\n",
    "            'stream': kwargs.get('stream', False)\n",
    "        }\n",
    "    elif model_type == \"llama\":\n",
    "        url = f\"{basicUrl}/deployments/{model_name}/llama/completion/?api-version={api_version}\"\n",
    "        payload = {\n",
    "            'messages': [{\"role\": \"user\", \"content\": message}],\n",
    "            'temperature': kwargs.get('temperature', 0),\n",
    "            'max_tokens': kwargs.get('max_tokens', 100),\n",
    "            'top_p': kwargs.get('top_p', 1.0),\n",
    "            'top_k': kwargs.get('top_k', 50),\n",
    "            'stop_sequences': kwargs.get('stop_sequences', None),\n",
    "            'stream': kwargs.get('stream', False),\n",
    "            'system': kwargs.get('system', None)\n",
    "        }\n",
    "        if kwargs.get('imageURL'):\n",
    "             payload['messages'][0]['content'] = [\n",
    "                {\"type\": \"text\", \"text\": message},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": kwargs.get('imageURL'), \"detail\": \"low\"}}\n",
    "            ]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"candidates\": [\n",
      "        {\n",
      "            \"content\": {\n",
      "                \"role\": \"model\",\n",
      "                \"parts\": [\n",
      "                    {\n",
      "                        \"text\": \"Hello there! How can I\"\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"finishReason\": \"MAX_TOKENS\",\n",
      "            \"avgLogprobs\": -0.0011572965110341709,\n",
      "            \"index\": 0\n",
      "        }\n",
      "    ],\n",
      "    \"usageMetadata\": {\n",
      "        \"promptTokenCount\": 1,\n",
      "        \"candidatesTokenCount\": 6,\n",
      "        \"totalTokenCount\": 7,\n",
      "        \"promptTokensDetails\": [\n",
      "            {\n",
      "                \"modality\": \"TEXT\",\n",
      "                \"tokenCount\": 1\n",
      "            }\n",
      "        ],\n",
      "        \"candidatesTokensDetails\": [\n",
      "            {\n",
      "                \"modality\": \"TEXT\",\n",
      "                \"tokenCount\": 6\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"modelVersion\": \"gemini-1.5-flash-002\",\n",
      "    \"createTime\": \"2025-02-21T10:34:07.417269Z\",\n",
      "    \"responseId\": \"n1a4Z_W7Gc347OsPtJaR8QQ\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "result = query_model(\n",
    "    message=\"hello\", \n",
    "    model_name=\"gemini-1.5-flash\", \n",
    "    kwargs={\n",
    "        'temperature':0.5, \n",
    "        'maxOutputTokens':6\n",
    "    }\n",
    ")\n",
    "print(json.dumps(result, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-B3Kf8D9sbonQIVUakIV7uCtvI5Oaw\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1740134110,\n",
      "    \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Hello! How can I assist\",\n",
      "                \"refusal\": null\n",
      "            },\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"length\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 8,\n",
      "        \"completion_tokens\": 6,\n",
      "        \"total_tokens\": 14,\n",
      "        \"prompt_tokens_details\": {\n",
      "            \"cached_tokens\": 0,\n",
      "            \"audio_tokens\": 0\n",
      "        },\n",
      "        \"completion_tokens_details\": {\n",
      "            \"reasoning_tokens\": 0,\n",
      "            \"audio_tokens\": 0,\n",
      "            \"accepted_prediction_tokens\": 0,\n",
      "            \"rejected_prediction_tokens\": 0\n",
      "        }\n",
      "    },\n",
      "    \"system_fingerprint\": \"fp_b705f0c291\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = query_model(\n",
    "    message=\"hello\", \n",
    "    model_name=\"gpt-4-o-mini\", \n",
    "    kwargs={\n",
    "        'temperature':0.5, \n",
    "        'max_tokens':6\n",
    "    }\n",
    "    )\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"Hello! How are you today? Is there something I can help you with or would you like to chat?\",\n",
      "                \"role\": \"assistant\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1740134170,\n",
      "    \"id\": \"2025-02-21|02:36:10.406190-08|2.24.218.11|50824290\",\n",
      "    \"model\": \"meta/llama-3.1-405b-instruct-maas\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"system_fingerprint\": \"\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 22,\n",
      "        \"prompt_tokens\": 1,\n",
      "        \"total_tokens\": 23\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = query_model(\n",
    "    message=\"hello\", \n",
    "    model_name=\"llama3_1\", \n",
    "    kwargs={\n",
    "        'temperature':0.5, \n",
    "        'max_tokens':6\n",
    "    }\n",
    "    )\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 429\n",
      "Response: {\"error\":{\"message\":\"Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: anthropic-claude-3-haiku. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\",\"param\":null,\"code\":429}}\n",
      "null\n"
     ]
    }
   ],
   "source": [
    "result = query_model(\n",
    "    message=\"hello\", \n",
    "    model_name=\"claude-3-haiku\", \n",
    "    kwargs={\n",
    "        'temperature':0.5, \n",
    "        'max_tokens':6\n",
    "    }\n",
    "    )\n",
    "print(json.dumps(result, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hkbu_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
